# -*- coding: utf-8 -*-
"""Copy of Final Project Notebook Hahn Stats 24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QH0pYsQFNmSCpW8duH7A_73N6HF7LF6n

# Final Project Notebook - Spring 2024
"""

import matplotlib.pyplot as plt
import imageio
import torch
import torchvision
from torchvision import models, transforms
import numpy as np
from torchvision.models import *
from PIL import Image
import requests
from torchvision import models
from torchsummary import summary

def plot(x):
    fig, ax = plt.subplots()
    im = ax.imshow(x,cmap='gray')
    ax.axis('off')
    fig.set_size_inches(20, 20)
    plt.show() #normal plot function, setting the size to 20x20

im = imageio.imread('https://raw.githubusercontent.com/imageio/imageio-binaries/master/images/imageio_banner.png') #read images from this url

plot(im) #plot the read images

net = alexnet(pretrained=True).cuda(0) #loads a pretrained model from alexnet. pretrained=true loads the model weights trained on the imagenet dataset

normalize = transforms.Normalize(
   mean=[0.485, 0.456, 0.406],
   std=[0.229, 0.224, 0.225]  #normalize the data set of each channel by subtracting the mean and dividing by the standard deviation
)
preprocess = transforms.Compose([
   transforms.Resize(256),
   transforms.CenterCrop(224),
   transforms.ToTensor(),
   normalize #transform the images by resizing, center cropping, and turning them into tensors
])

im = imageio.imread('https://www.medicalnewstoday.com/content/images/articles/322/322868/golden-retriever-puppy.jpg')

plot(im)

image = Image.fromarray(im) #convert to pil

img_tensor = preprocess(image) #preprocess the image based on the above function

img_tensor = img_tensor.unsqueeze_(0) #adds a dimension to the front of the tensor

img_tensor.shape

img_variable = torch.tensor(img_tensor).cuda(0) #converts the image tensor into a pytorch tensory and moves it to the gpu device

out = net(img_variable) #runs the image variable into the neural network model "net" and creates a tensory containing the output data

label_index = out.cpu().data.numpy().argmax() #converts the output into an array, finds the max index, and stores that result

label_index

top_list = np.flip(np.argsort(out.cpu().data.numpy())[0][-10:]) #sorts the output tensor into descending order and takes the highest 10 probabilites and stores those into the variable "top_list"

LABELS_URL = 'https://s3.amazonaws.com/mlpipes/pytorch-quick-start/labels.json'

labels = {int(key):value for (key, value) in requests.get(LABELS_URL).json().items()} #retrieves class labels from a remote file and converts the keys to integers and stores them in a dictionary named "labels"

print(labels[label_index]) #prints the label corresponding to the predicted index obtained from the model output

for i in range(10):
    print(labels[top_list[i]]) #prints the labels generated by the top list function

net #prints data from the model

summary(net, (3, 224, 224)) #takes a summary of the data from the model, based on an inputted shape (3x224x224)

out = net.features[0](img_variable).cpu().detach().numpy() #applies the first convulation layer to the inputted image variable

plot(out[0,0,:,:]) #plots the first output feature of the first example in the batch

plt.plot(np.arange(4096),net.classifier[0:6](net.avgpool(net.features[0:13](img_variable)).flatten()).cpu().detach().numpy()) #passes the input image tensor throught the first 13 convultional layers of the model
#selects the first 6 layers of the classifier part of the model, applies an average pooling to the features extracted by the convultional layers, flattens the output into a 1d tensor, plots the flattened output of the classifier against an array of indices from 0-4095, and converts the tensor to a numpy array
fig = plt.gcf() #retrieves the current figure
fig.set_size_inches(10, 10) #set the figure to 10x10 inches

im = imageio.imread('http://bocasurfcam.com/most_recent_image.php')

plot(im)

def load_im(im):
    image = Image.fromarray(im) #convert to pil
    img_tensor = preprocess(image) #preprocess the image for input into the model
    img_tensor = img_tensor.unsqueeze_(0) #adds an extra dimension to the tensor
    img_variable = torch.tensor(img_tensor).cuda(0) #converts the image tensor to a pytorch tensor and moves it to the gpu memory
    return img_variable

out = net(load_im(im)) #run the image into the model and generate an output tensor

def inference(im):
    out = net(load_im(im))
    label_index = out.cpu().data.numpy().argmax()
    top_list = np.flip(np.argsort(out.cpu().data.numpy())[0][-10:])
    print(labels[label_index])
    print('____')
    for i in range(10):
         print(labels[top_list[i]]) #perform all of the above operators in one function to input an image into the model and generate a lsit of the 10 best predicitons

inference(im)

"""# Restart Notebook (Disconnect and Delete Runtime) Before Running Next Section

# Custom Data Deck
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install wandb
# !apt-get install poppler-utils
# !pip install pdf2image
# !pip install flashtorch
# import requests
# from pdf2image import convert_from_path
# import matplotlib.pyplot as plt
# import numpy as np
# import torch
# import requests
# from torchvision import *
# from torchvision.models import *
# from flashtorch.utils import apply_transforms
# import wandb as wb
# 
# 
# #i do not have a gpu and i keep getting error messages trying to run the code, I will just annotate what is here and do my best to follow along

def GPU(data):
    return torch.tensor(data, requires_grad=True, dtype=torch.float, device=torch.device('cuda'))

def GPU_data(data):
    return torch.tensor(data, requires_grad=False, dtype=torch.float, device=torch.device('cuda'))  #same gpu functions as before

def plot(x):
    fig, ax = plt.subplots()
    im = ax.imshow(x, cmap = 'gray')
    ax.axis('off')
    fig.set_size_inches(5, 5)
    plt.show() #same plot function as before

def get_google_slide(url):
    url_head = "https://docs.google.com/presentation/d/"
    url_body = url.split('/')[5]
    page_id = url.split('.')[-1]
    return url_head + url_body + "/export/pdf?id=" + url_body + "&pageid=" + page_id #same google slide import function as before

def get_slides(url):
    url = get_google_slide(url)
    r = requests.get(url, allow_redirects=True)
    open('file.pdf', 'wb').write(r.content)
    images = convert_from_path('file.pdf', 500)
    return images #convert the pdf file to images

def load(image):

    return apply_transforms(image).clone().detach().requires_grad_(True).to(device) #transform the images

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

labels = {int(key):value for (key, value) in requests.get('https://s3.amazonaws.com/mlpipes/pytorch-quick-start/labels.json').json().items()} #obtain the label set from this online hosted set of labels

model = alexnet(weights='DEFAULT').to(device) #loads an alexnet model using the default weights on the specified device
model.eval();  #sets the model into evaluation mode, different than training mode

url = "https://docs.google.com/presentation/d/1lyE52a0yLCu9iUiKeFmdKayGJhu2Z6fyToXEMKeigoY/edit#slide=id.p"

images = [] #same function to grab the images from the google slide and stack them vertically as tensors

for image in get_slides(url):

    plot(image)

    images.append(load(image))

images = torch.vstack(images)

images.shape

model(images) #shows the outputted probabilites of the model

y = model(images)

y.shape

guesses = torch.argmax(y, 1).cpu().numpy() #find the highest value in the guess probability and record its index

for i in list(guesses):
    print(labels[i]) #print the guesses for each image given by the model

Y = np.zeros(50,)
Y[25:] = 1 #initialize an array of 50 values, 0-24 zero and 25-49, 1

Y

X = y.detach().cpu().numpy() #extract the values from the tensor y, move it to the cpu, and convert it to a numpy array

X.shape

plt.plot(X[0],'.') #plot the results of the first channel of the outputs (the probabilites) of each guess

X[0]

np.argmax(X[0]) #find the index of the greatest value

labels[948]

top_ten = np.argsort(X[0])[::-1][0:10] #make a top 10 list of guessses

for i in top_ten:
    print(labels[i])

labels #list all of the labels

plt.hist(X[0]) #apply a histogram for the guesses made on the first image, the data resembles a normal distrubiton

X = GPU_data(X)
Y = GPU_data(Y) #this is where i run into error problems

def softmax(x):
    s1 = torch.exp(x - torch.max(x,1)[0][:,None])
    s = s1 / s1.sum(1)[:,None]
    return s

def cross_entropy(outputs, labels):
    return -torch.sum(softmax(outputs).log()[range(outputs.size()[0]), labels.long()])/outputs.size()[0]

def Truncated_Normal(size):

    u1 = torch.rand(size)*(1-np.exp(-2)) + np.exp(-2)
    u2 = torch.rand(size)
    z  = torch.sqrt(-2*torch.log(u1)) * torch.cos(2*np.pi*u2)

    return z

def acc(out,y):
    with torch.no_grad():
        return (torch.sum(torch.max(out,1)[1] == y).item())/y.shape[0] #same functions as before

X.shape

def get_batch(mode):
    b = c.b
    if mode == "train":
        r = np.random.randint(X.shape[0]-b)
        x = X[r:r+b,:]
        y = Y[r:r+b]
    elif mode == "test":
        r = np.random.randint(X_test.shape[0]-b)
        x = X_test[r:r+b,:]
        y = Y_test[r:r+b]
    return x,y

def model(x,w):

    return x@w[0]

def make_plots():

    acc_train = acc(model(x,w),y)

    wb.log({"acc_train": acc_train}) #same functions as before

wb.init(project="Linear_Model_Photo_1");
c = wb.config

c.h = 0.001
c.b = 4
c.epochs = 100000

w = [GPU(Truncated_Normal((1000,2)))]

optimizer = torch.optim.Adam(w, lr=c.h)

for i in range(c.epochs):

    x,y = get_batch('train')

    loss = cross_entropy(softmax(model(x,w)),y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    wb.log({"loss": loss})

    make_plots() #optimizer for the model, same one used earlier





