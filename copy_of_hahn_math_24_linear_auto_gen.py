# -*- coding: utf-8 -*-
"""Copy of Hahn Math 24 Linear Auto Gen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YR0VNI5dglyd_2uvE9r3OE5YxByEfbB0

## Setup

### Imports
"""

import numpy as np
import matplotlib.pyplot as plt
import urllib.request
from PIL import Image
from imageio import *
import torch
from skimage.transform import resize
from mpl_toolkits.axes_grid1.axes_rgb import make_rgb_axes, RGBAxes
from torchvision.models import *
from torchvision.datasets import MNIST,KMNIST,FashionMNIST
from skimage.util import montage

!pip install wandb
import wandb as wb

def plot(x):
    if type(x) == torch.Tensor :
        x = x.cpu().detach().numpy()

    fig, ax = plt.subplots()
    im = ax.imshow(x, cmap = 'gray')
    ax.axis('off')
    fig.set_size_inches(5, 5)
    plt.show() #sets up the plot function and converts tesnsors to arrays if applicable

def montage_plot(x):
    x = np.pad(x, pad_width=((0, 0), (1, 1), (1, 1)), mode='constant', constant_values=0)
    plot(montage(x)) #define a function to plot a montage of the inputted tensors

b = 1000  #batch size of 1000

def get_batch(mode):
    if mode == "train":
        r = np.random.randint(X.shape[0]-b)
        x = X[r:r+b,:]
        y = Y[r:r+b]
    elif mode == "test":
        r = np.random.randint(X_test.shape[0]-b)
        x = X_test[r:r+b,:]
        y = Y_test[r:r+b]
    return x,y

"""## MNIST

### Load Data
"""

# #MNIST
train_set = MNIST('./data', train=True, download=True)
test_set  = MNIST('./data', train=False, download=True)

#KMNIST
# train_set = KMNIST('./data', train=True, download=True)
# test_set =  KMNIST('./data', train=False, download=True)

# Fashion MNIST
# train_set = FashionMNIST('./data', train=True, download=True)
# test_set =  FashionMNIST('./data', train=False, download=True)

X = train_set.data.numpy()
X_test = test_set.data.numpy()
Y = train_set.targets.numpy()
Y_test = test_set.targets.numpy()

X = X[:,None,:,:]/255 #normalize the pixel data for X
X_test = X_test[:,None,:,:]/255 #normalize the pixel data for Xtest

X.shape

Y[50000] #training label for the 50001st element

plot(X[50000,0,:,:]) #plot that element

Y[100]

X.shape

X[0:25,0,:,:].shape #first 25 images in the training set

montage_plot(X[125:150,0,:,:])

X.shape[0]

X_test.shape #shape of the test data

X.shape[0]

X_test.shape[0]

def GPU(data):
    return torch.tensor(data, requires_grad=True, dtype=torch.float, device=torch.device('cuda')) #same function as earlier lab, however no cpu fallback

def GPU_data(data):
    return torch.tensor(data, requires_grad=False, dtype=torch.float, device=torch.device('cuda'))

X = GPU_data(X) #runs above function
Y = GPU_data(Y)
X_test = GPU_data(X_test)
Y_test = GPU_data(Y_test)

X = X.reshape(X.shape[0],784)
X_test = X_test.reshape(X_test.shape[0],784)

X.shape

"""
### Classifier
"""

x,y = get_batch('train') #retrieves a batch of training data using the random batch feature

x.shape #shape of the retrieved image data

plot(x[0].reshape(28,28)) #reshape the first element as a 28x28 array

plot(x[1].reshape(28,28)) #second element

plot(x[2].reshape(28,28)) #third element

y[:10] #display the labels for the first 10 elements

W = GPU(np.random.randn(784,10)) #creates a random set of weight parameters



x.shape, W.shape #shape of the two tensors

torch.matmul(x,W).shape #matrix multiplatication

(x@W).shape #shape of that multiplication

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# x@W #time it takes to calculate

x@W

y2 = x@W

plot(y2[:50]) #plot the first 50 results from the matrix mult.

y

y.shape #shape of the label training dataset

def one_hot(y):
    y2 = GPU_data(torch.zeros((y.shape[0],10)))
    for i in range(y.shape[0]):
        y2[i,int(y[i])] = 1
    return y2 #returns a tensor where each row represents a unique representation of a class label, with a 1 indictating the label, and all other elements 0

one_hot(y)

torch.argmax(y2,1) #finds the index of the max for the one hot tensor

torch.sum(y == torch.argmax(y2,1))/b #calculates the accuracy of a batch of predicitons based on the known labels "y"

X.shape

X@W

torch.argmax(X@W,1) #finds the max of the training data matrix multiplied by the weights

Y

torch.sum(torch.argmax(X@W,1) == Y)/60000 #find the accuracy of the whole model

X@W

W.shape

W[:,0].shape

plot(W[:,0].reshape(28,28)) #reshape the second column weights paramaters to a 28x28 matrix and plot it

plot(W[:,2].reshape(28,28)) #does the same as above but for the 3rd column of the weights parameter

W.shape

(W.T).shape #transpose the dimensions and show the shape

montage_plot((W.T).reshape(10,28,28).cpu().detach().numpy()) #show a montage plot of the W parameter

def softmax(x): #same functions as last lab, used to find the accuracy
    s1 = torch.exp(x - torch.max(x,1)[0][:,None])
    s = s1 / s1.sum(1)[:,None]
    return s

def cross_entropy(outputs, labels):
    return -torch.sum(softmax(outputs).log()[range(outputs.size()[0]), labels.long()])/outputs.size()[0]

def acc(out,y):
    return (torch.sum(torch.max(out,1)[1] == y).item())/y.shape[0]

def get_batch(mode):
    b = c.b
    if mode == "train":
        r = np.random.randint(X.shape[0]-b)
        x = X[r:r+b,:]
        y = Y[r:r+b]
    elif mode == "test":
        r = np.random.randint(X_test.shape[0]-b)
        x = X_test[r:r+b,:]
        y = Y_test[r:r+b]
    return x,y

def model(x,w):

    return x@w[0]

def gradient_step(w):

    w[0].data = w[0].data - c.L*w[0].grad.data

    w[0].grad.data.zero_() #updates the parameter "w" by performing a single step of gradient descent based on the models learning rate. After updating the parameter, gradients are set to zero to prepare for next iteration

def make_plots():

    acc_train = acc(model(x,w),y)

    xt,yt = get_batch('test')

    acc_test = acc(model(xt,w),yt)

    wb.log({"acc_train": acc_train, "acc_test": acc_test})

def Truncated_Normal(size):

    u1 = torch.rand(size)*(1-np.exp(-2)) + np.exp(-2)
    u2 = torch.rand(size)
    z  = torch.sqrt(-2*torch.log(u1)) * torch.cos(2*np.pi*u2)

    return z #same functions as last lab

for run in range(3): #implements a training loop for a linear model using stochastic gradient descent (SGD)

    wb.init(project="Simple_Linear_SGD_123"); #initialize a new set of weights
    c = wb.config #edit the parameters of the model

    c.L = 0.1 #learning rate
    c.b = 1024 #batch size
    c.epochs = 10000 #number of epochs

    w = [GPU(Truncated_Normal((784,10)))] #create a random normal distrubution with a size of 784x10

    for i in range(c.epochs): #for each epoch

        x,y = get_batch('train') #get training data from the get batch function

        out = model(x,w) #obtain the models predictions

        loss = cross_entropy(softmax(out),y) #calculate the loss of each element against the true labels "y"

        loss.backward() #gradients of the loss with respect to the model parameters are computed

        gradient_step(w) #performs the gradient step function on the weights w

        make_plots()

        if (i+1) % 10000 == 0: montage_plot((w[0].T).reshape(10,28,28).cpu().detach().numpy()) #plot the montage of the weights parameter every 10000 epochs

for run in range(100): #runs same as above but 100 times

    wb.init(project="Simple_Linear_Adam_2");
    c = wb.config

    c.L = 0.01
    c.b = 1024
    c.epochs = 100000

    w = [GPU(Truncated_Normal((784,10)))]

    optimizer = torch.optim.Adam(w, lr=c.L)

    for i in range(c.epochs):

        x,y = get_batch('train')

        loss = cross_entropy(softmax(model(x,w)),y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        wb.log({"loss": loss})

        make_plots()

        if i % 10000 == 0 : montage_plot((w[0].T).reshape(10,28,28).cpu().detach().numpy())

"""
### Autoencoder
"""

def get_batch(mode): #get batch function with set b of 1024
    b = 1024
    if mode == "train":
        r = np.random.randint(X.shape[0]-b)
        x = X[r:r+b,:]
        y = Y[r:r+b]
    elif mode == "test":
        r = np.random.randint(X_test.shape[0]-b)
        x = X_test[r:r+b,:]
        y = Y_test[r:r+b]
    return x,y

X = X.reshape(X.shape[0],1,28,28)
X_test = X_test.reshape(X_test.shape[0],1,28,28) #reshape both arrays

import torchvision
from torch.nn.functional import *

X = torchvision.transforms.functional.normalize(X,0.5,0.5) #normalize each tensor with a mean value of 0.5 and a standard deviation of 0.5
X_test = torchvision.transforms.functional.normalize(X_test,0.5,0.5)

def Encoder(x,w): #encondes an inputted image to transform the tensor into a lower dimensional one using convolution and fully connected layers
    x = relu(conv2d(x,w[0], stride=(2, 2), padding=(1, 1))) #applies a 2d convuliton to x using the w[0] parameters.
    x = relu(conv2d(x,w[1], stride=(2, 2), padding=(1, 1))) #same but using the w[1] parameter
    x = x.view(x.size(0), 6272) #reshape the output tensor into a tensor of a specific size
    x = linear(x,w[2]) #applies a linear transformation to the reshaped tensor
    return x

def Decoder(x,w): #performs the reverse process as above, decoding the data into an image tensor that we can view in higher dimensions
    x = linear(x,w[3])
    x = x.view(x.size(0), 128, 7, 7)
    x = relu(conv_transpose2d(x,w[4], stride=(2, 2), padding=(1, 1)))
    x = torch.tanh(conv_transpose2d(x,w[5], stride=(2, 2), padding=(1, 1)))
    return x

def Autoencoder(x,w): #define a functon to perform both above functions in 1
    return Decoder(Encoder(x,w),w)

num_steps = 1000
batch_size = 512
learning_rate = 1e-3

from scipy import stats
import numpy as np
import matplotlib.pyplot as plt
import urllib.request
from PIL import Image
from imageio import *
import torch
from skimage.transform import resize
from mpl_toolkits.axes_grid1.axes_rgb import make_rgb_axes, RGBAxes
from torchvision.models import *
from torchvision.datasets import MNIST,KMNIST,FashionMNIST
from skimage.util import montage

def randn_trunc(s): #Truncated Normal Random Numbers
    mu = 0
    sigma = 0.1
    R = stats.truncnorm((-2*sigma - mu) / sigma, (2*sigma - mu) / sigma, loc=mu, scale=sigma)
    return R.rvs(s)

#Encode
w0 = GPU(randn_trunc((64,1,4,4))) #w0,1, and 2 reprsent the weights for the encoder layers
w1 = GPU(randn_trunc((128,64,4,4)))  #w0 and 1 are convulutional layers
w2 = GPU(randn_trunc((10,6272)))  #w2 is a fully connected layer
#Decode
w3 = GPU(randn_trunc((6272,10))) #w3 is a fully connected layer
w4 = GPU(randn_trunc((128,64,4,4))) #w4 and 5 are transpose convultional layers
w5 = GPU(randn_trunc((64,1,4,4))) #the weights for the decoder function

w = [w0,w1,w2,w3,w4,w5] #list of all the weights

optimizer = torch.optim.Adam(params=w, lr=learning_rate) #initialize the adam optimizer using the inputter parameters and learning rate

for i in range(num_steps): #1000 iterations (from above)

    x_real,y = get_batch('train') #takes real input data from the training set of data

    x_fake = Autoencoder(x_real,w) #encodes and decodes the input data to produce a reconstructed (fake) set of data

    loss = torch.mean((x_fake - x_real)**2) #calculate the loss of the encoder between the inputted and outputted data

    optimizer.zero_grad() #optimize the model using gradient steps
    loss.backward()
    optimizer.step()

    if i % 100 == 0: print(loss.item()) #print the loss every 100 steps

image_batch,y = get_batch('test') #get the testing data

image_batch_recon = Autoencoder(image_batch,w) #reconstruct the testing data using the autoencoder

torch.mean((image_batch_recon - image_batch)**2) #calculate the loss of the autoencoder

montage_plot(image_batch[0:25,0,:,:].cpu().detach().numpy()) #do a montage plot of the first 25 elements in the training data

montage_plot(image_batch_recon[0:25,0,:,:].cpu().detach().numpy()) #montage plot for the first 25 elements of the outputted reconstructed data from the autoencoder

"""
### Generator

"""



latent_size = 64 #the size of the encoded representations learned by the model
hidden_size = 256 #number of units in the   hidden layers of a neural network, determins the capacity of the model to learn
image_size = 784 #image size (pixels)
b = 1024 #batch size

#MNIST
# train_set = MNIST('./data', train=True, download=True)
# test_set = MNIST('./data', train=False, download=True)

#KMNIST
#train_set = KMNIST('./data', train=True, download=True)
#test_set = KMNIST('./data', train=False, download=True)

#Fashion MNIST
train_set = FashionMNIST('./data', train=True, download=True) #download the training data for the MNIST fashion data set clas
test_set = FashionMNIST('./data', train=False, download=True) #download the testing data for the MNIST fashion data set class

X = train_set.data.numpy() #convert to numpy array
X_test = test_set.data.numpy()
Y = train_set.targets.numpy()
Y_test = test_set.targets.numpy()
X = X[:,None,:,:]/255 #reshpae the arrays to have no second column, then divide the pixel values to normalize them
X_test = X_test[:,None,:,:]/255
X = (X - 0.5)/0.5 #scale the pixel ranges from between -1 to 1
X_test = (X_test - 0.5)/0.5

n = 7 #filter the dataset to only include images where the class label "n" is set to 7

index = np.where(Y == n)
X = X[index]
index = np.where(Y_test == n)
X_test = X_test[index]

X.shape,Y.shape,X_test.shape,Y_test.shape

###################################################

X = GPU_data(X)
X_test = GPU_data(X_test)
Y = GPU_data(Y)
Y_test = GPU_data(Y_test) #perform earlier functions on the dataset to prepare for input into the model

x,y = get_batch('train') #get the training batch



x.shape

montage_plot(x[0:25,0,:,:].detach().cpu().numpy()) #montage plot the first 25 elements of the tensor "x"

#D
w0 = GPU(randn_trunc((64,1,4,4)))
w1 = GPU(randn_trunc((128,64,4,4)))
w2 = GPU(randn_trunc((1,6272)))
#G
w3 = GPU(randn_trunc((6272,64)))
w4 = GPU(randn_trunc((128,64,4,4)))
w5 = GPU(randn_trunc((64,1,4,4)))

w = [w0,w1,w2,w3,w4,w5] #set up the weights for the autoencoder

def D(x,w): #encoder function
    x = relu(conv2d(x,w[0], stride=(2, 2), padding=(1, 1)))
    x = relu(conv2d(x,w[1], stride=(2, 2), padding=(1, 1)))
    x = x.view(x.size(0), 6272)
    x = linear(x,w[2])
    x = torch.sigmoid(x)
    return x

def G(x,w): #decoder function
    x = linear(x,w[3])
    x = x.view(x.size(0), 128, 7, 7)
    x = relu(conv_transpose2d(x,w[4], stride=(2, 2), padding=(1, 1)))
    x = torch.tanh(conv_transpose2d(x,w[5], stride=(2, 2), padding=(1, 1)))
    return x

b = 1024

batch_size = b

batch_size

d_optimizer = torch.optim.Adam(w[0:3], lr=0.0002) #encoder optimization
g_optimizer = torch.optim.Adam(w[3:], lr=0.0002) #decoder optimization

real_labels = (torch.ones(batch_size, 1).cuda())  #creating labels for the real samples using a binary tensor
fake_labels = (torch.zeros(batch_size, 1).cuda()) #creating labels for the reconstructed samples using a binary tensor

num_epochs = 500
batches = X.shape[0]//batch_size
steps = num_epochs*batches

z1 = (torch.randn(steps,batch_size,latent_size).cuda()) #generating random noise vectors for input into the model
z2 = (torch.randn(steps,batch_size,latent_size).cuda())

for i in range(steps): #training loop that alternates between optimzing the encoder (discriminator) and the decoder (generator)

    images,y = get_batch('train')

    d_loss = binary_cross_entropy(D(images,w), real_labels) + binary_cross_entropy(D(G(z1[i],w),w), fake_labels) #loss of the encoder
    d_optimizer.zero_grad() #optimize the encoder weights
    d_loss.backward()
    d_optimizer.step()


    g_loss = binary_cross_entropy(D(G(z2[i],w),w), real_labels) #loss of the decoder
    g_optimizer.zero_grad() #optimize the decoder weights
    g_loss.backward()
    g_optimizer.step()


    if i % 200 == 0:
        out = G(z1[np.random.randint(steps)],w) #every 200 iterations, output the reconstructed images generated by the autoencoder.
        montage_plot(out.view(batch_size,1,28,28).detach().cpu().numpy()[0:25,0,:,:]) #plot the first 25 of these in a montage plot





z1[np.random.randint(steps)].shape #selects a random noise vector from first dimension of the tensor z1

noise = GPU_data(torch.randn(1,64)) #generates a random 1x64 vector to represnet noise

output = G(noise,w) #run the generator on the inputted noise

output.shape

plot(output[0,0]) #plot the reconstruction of the noise



















