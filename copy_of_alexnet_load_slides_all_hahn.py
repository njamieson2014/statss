# -*- coding: utf-8 -*-
"""Copy of Alexnet Load Slides All Hahn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mFoI_wm6d_pAJhPYR4uRpbp8HFYOtqnS
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install wandb
# !apt-get install poppler-utils
# !pip install pdf2image
# !pip install flashtorch
# import requests
# from pdf2image import convert_from_path
# import matplotlib.pyplot as plt
# import numpy as np
# import torch
# import requests
# from torchvision import *
# from torchvision.models import *
# import wandb as wb

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") #assigns device to be used as CUDA. if cuda is not available, it will use the cpu

def GPU(data):
    return torch.tensor(data, requires_grad=True, dtype=torch.float, device=device) #creates a pytorch tensor with the inputted data and the specified device (from the first line)

def GPU_data(data):
    return torch.tensor(data, requires_grad=False, dtype=torch.float, device=device) #creates a pytorch tensor with the inputted data using the specified device, but without computing the gradient during optimization. This is useful for data tensors that are not involved in model training

def plot(x):
    fig, ax = plt.subplots()
    im = ax.imshow(x, cmap = 'gray')
    ax.axis('off')
    fig.set_size_inches(5, 5)
    plt.show()  #creates a plot using an input array "x" in grayscale and 5inches by inches

def get_google_slide(url): #takes a google slides presentation url and converts it into a pdf
    url_head = "https://docs.google.com/presentation/d/"
    url_body = url.split('/')[5] #adds the url of the presentation to the above link
    page_id = url.split('.')[-1]
    return url_head + url_body + "/export/pdf?id=" + url_body + "&pageid=" + page_id

def get_slides(url): #takes the slides from the presentation and converts them to images and then displays the images using the pdf created above
    url = get_google_slide(url)
    r = requests.get(url, allow_redirects=True)
    open('file.pdf', 'wb').write(r.content)
    images = convert_from_path('file.pdf', 500)
    return images

def load(image, size=224):
    means = [0.485, 0.456, 0.406]
    stds = [0.229, 0.224, 0.225]
    transform = transforms.Compose([
        transforms.Resize(size),
        transforms.CenterCrop(size),
        transforms.ToTensor(),
        transforms.Normalize(means, stds)
    ])
    tensor = transform(image).unsqueeze(0).to(device)
    tensor.requires_grad = True
    return tensor #this function proccesses an inputted image by resizing, center cropping, converting it to a tensor, normalizing it, and adding another dimension to it. This is used to make the image suitable to input into a nueral network

labels = {int(key):value for (key, value) in requests.get('https://s3.amazonaws.com/mlpipes/pytorch-quick-start/labels.json').json().items()}

model = alexnet(weights='DEFAULT').to(device)
model.eval(); #loads a pre-trained alexnet model and and a set of labels for classifying the images, on the specified device

url = "https://docs.google.com/presentation/d/17Nxy2Wo0erk71fp4sCDQqHrHYkxwjdExjbosjoEewuU/edit#slide=id.p"

images = [] #initializes an empty list to store the images from the google presnetation

for image in get_slides(url): #iterates for each slide in the presentation

    plot(image) #plots the current image

    images.append(load(image)) #process the image using the above "load" function and appends it to the list "images"

images = torch.vstack(images) #vertically stacks the list of images into a single tensor

images.shape #50 images, 3 color channels, dimensions

model(images) #using the alexnet model to return a tesnsor containing the predicted probabilities for each image class

y = model(images)

y.shape

guesses = torch.argmax(y, 1).cpu().numpy() #extracts the predicted results from the alexnet model and stores them in a numpy array named "guesses"

for i in list(guesses):
    print(labels[i]) #goes through the lsit of predicted labels and prints each one

Y = np.zeros(50,)
Y[25:] = 1 #sets up an array with 0's from 0-24 and 1's from 25-50

Y

# Y = np.zeros(100,)
# Y[50:] = 1

Y

X = y.detach().cpu().numpy() #takes a tensory y, and converts it to a numpy array X

X.shape

plt.plot(X[0],'.') #plot the guesses of the alexnet

plt.hist(X[0]) #histogram of the data

X = GPU_data(X)
Y = GPU_data(Y) #turns both arrays into tensors

def softmax(x):
    s1 = torch.exp(x - torch.max(x,1)[0][:,None])
    s = s1 / s1.sum(1)[:,None]
    return s #creates a softmax function, normalizing the values of the tensor so that all of the exponent values in a given row add up to 1

def cross_entropy(outputs, labels):
    return -torch.sum(softmax(outputs).log()[range(outputs.size()[0]), labels.long()])/outputs.size()[0] #takes the data from the model and compares it inputted true data, and calculates the cross entropy between them, or the average loss per sample
    #this can give us a numeric representation of how right or wrong the model is

def randn_trunc(s): #Truncated Normal Random Numbers
    mu = 0
    sigma = 0.1
    R = stats.truncnorm((-2*sigma - mu) / sigma, (2*sigma - mu) / sigma, loc=mu, scale=sigma)
    return R.rvs(s) #returns an array in the shape of "s", a set of random numbers whose mean is 0 and standard deviation is 0.1

def Truncated_Normal(size):

    u1 = torch.rand(size)*(1-np.exp(-2)) + np.exp(-2)
    u2 = torch.rand(size)
    z  = torch.sqrt(-2*torch.log(u1)) * torch.cos(2*np.pi*u2)

    return z #comes up with a normal distribution of numbers between e^-2 and 1. This avoids numbers close to zero that would impact a logarithm calculation performed later
    #creates a normal distrubtion within a certain range, can be useful for sampling

def acc(out,y):
    with torch.no_grad(): #turns gradients off, can improve performance
        return (torch.sum(torch.max(out,1)[1] == y).item())/y.shape[0] #caculates the accuracy of the model predictions against the inputted "y" values (the true values)

X.shape

def get_batch(mode): #defines a function to get a batch of data samples and their corresponding data labels
    b = c.b #retrieves the batch size "b" from the variable "c.b"
    if mode == "train": #if mode is set to training mode
        r = np.random.randint(X.shape[0]-b) #generates a random integer between 0 and the size of the dataset "X"
        x = X[r:r+b,:] #extracts the sample data from the X tensor starting from the random index "r" and grabbing "b" amount of samples
        y = Y[r:r+b] #does the same for labels
    elif mode == "test": #otherwise set to ttesting modde
        r = np.random.randint(X_test.shape[0]-b)
        x = X_test[r:r+b,:] #do the same as above but with a different sample batch (one to test the model on)
        y = Y_test[r:r+b]
    return x,y

def model(x,w):

    return x@w[0] #performs matrix multiplication on the input data "x" and the weight "w" of each image. returns the resulting output of the multiplication when running the model function

def make_plots():

    acc_train = acc(model(x,w),y) #calls the accuracy function to compute the accuracy of the model by using its output and the true values "y"

    # xt,yt = get_batch('test')

    # acc_test = acc(model(xt,w),yt)

    wb.log({"acc_train": acc_train}) #logs the accuracy results using the weights and bias tool, which helps us observe the performance of the model

wb.init(project="Linear_Model_Photo_1"); #initializs a weights and bias project named linear model photo 1.
c = wb.config #retrieves the configuration data from weight and biases which allows access to the projects configuration parameters

c.h = 0.001 #learning rate
c.b = 32 #batch size
c.epochs = 100 #number of training epochs

w = [GPU(Truncated_Normal((1000,2)))] #creates a normal distrubtion with a shape 1000x2. starts this out as the model weight

optimizer = torch.optim.Adam(w, lr=c.h) #initializes the adam optimizer using the learning rate "c.h"

for i in range(c.epochs): #iterates for the # of epochs specified

    x,y = get_batch('train') #performs the get batch function to get a randomly selected batch of training data

    loss = cross_entropy(softmax(model(x,w)),y) #perform the cross entropy function to calcuate the average loss of each result

    optimizer.zero_grad() #clears the gradients of all tensors
    loss.backward() #computes the gradients of the of the loss of all that paramters that have "requires_grad=True"
    optimizer.step() #updates the parameters of the model using the gradients computed above

    wb.log({"loss": loss}) #log the loss using weights and biases

    make_plots() #compute and log the training accuracy



#cannot run this function, i get this error  Paste an API key from your profile and hit enter, or press ctrl+c to quit:
#will not let me proceed





















